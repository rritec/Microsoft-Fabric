{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5faaf572",
   "metadata": {},
   "source": [
    "# Kappa Architecture Implementation in Microsoft Fabric using PySpark\n",
    "\n",
    "This notebook demonstrates a simplified Kappa Architecture using a publicly available streaming dataset, simulating ingestion, processing, storage, and serving in a Microsoft Fabric-like environment.\n",
    "\n",
    "> Dataset: [NYC Taxi Trips Stream Sample](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page) - We'll simulate streaming using `readStream` on a folder with taxi trip files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb801731",
   "metadata": {},
   "source": [
    "## ✅ 1. Simulate Streaming Ingestion from NYC Taxi Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e17c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"KappaDemo\").getOrCreate()\n",
    "\n",
    "# Define source folder with streaming files (simulate stream)\n",
    "source_path = \"/mnt/nyc/taxi_stream_input\"\n",
    "\n",
    "# Read streaming data (simulate JSON stream)\n",
    "df_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"json\")\n",
    "    .schema(\"VendorID INT, tpep_pickup_datetime TIMESTAMP, tpep_dropoff_datetime TIMESTAMP, passenger_count INT, trip_distance DOUBLE, fare_amount DOUBLE\")\n",
    "    .load(source_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c76c1f",
   "metadata": {},
   "source": [
    "## ✅ 2. Store Raw Stream in Delta Table (Immutable Log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b5ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store raw streaming data to Delta Lake (OneLake equivalent)\n",
    "raw_output_path = \"/mnt/delta/raw_nyc_taxi\"\n",
    "\n",
    "query_raw = (\n",
    "    df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/checkpoints/raw_nyc\")\n",
    "    .start(raw_output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87606c9f",
   "metadata": {},
   "source": [
    "## ✅ 3. Process Streaming Data: Filter and Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65feb228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for trips with distance > 2 miles\n",
    "df_filtered = df_stream.filter(\"trip_distance > 2\")\n",
    "\n",
    "# Aggregate by hour and passenger count\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "df_agg = (\n",
    "    df_filtered.groupBy(window(\"tpep_pickup_datetime\", \"1 hour\"), \"passenger_count\")\n",
    "    .count()\n",
    "    .withColumnRenamed(\"count\", \"trip_count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5f7eaa",
   "metadata": {},
   "source": [
    "## ✅ 4. Write Processed Data to Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_output_path = \"/mnt/delta/processed_nyc_taxi\"\n",
    "\n",
    "query_processed = (\n",
    "    df_agg.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/checkpoints/processed_nyc\")\n",
    "    .start(processed_output_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fd7df6",
   "metadata": {},
   "source": [
    "## ✅ 5. Serve Processed Data with SQL (Lakehouse SQL Endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Register table (if using SQL)\n",
    "CREATE TABLE IF NOT EXISTS processed_nyc_taxi\n",
    "USING DELTA\n",
    "LOCATION '/mnt/delta/processed_nyc_taxi';\n",
    "\n",
    "-- Query processed data\n",
    "SELECT *\n",
    "FROM processed_nyc_taxi\n",
    "ORDER BY trip_count DESC\n",
    "LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26682c4f",
   "metadata": {},
   "source": [
    "## ✅ 6. Reprocessing Historical Data (Kappa Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b861b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reprocess from raw data with new logic\n",
    "df_reprocess = spark.read.format(\"delta\").load(raw_output_path)\n",
    "\n",
    "# New transformation: filter by fare_amount > 50\n",
    "df_reprocessed = df_reprocess.filter(\"fare_amount > 50\")\n",
    "\n",
    "# Show high-fare trips\n",
    "df_reprocessed.select(\"tpep_pickup_datetime\", \"trip_distance\", \"fare_amount\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45cfe6d",
   "metadata": {},
   "source": [
    "## ✅ Stop All Streaming Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93c891",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in spark.streams.active:\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bceb57f",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "\n",
    "- Ingested NYC taxi data as a simulated stream\n",
    "- Stored raw data in Delta Lake as immutable log\n",
    "- Processed and aggregated real-time data\n",
    "- Queried final output for reporting\n",
    "- Reprocessed historical data from log — no duplicate logic\n",
    "\n",
    "This aligns with **Kappa Architecture principles**, implemented using **PySpark** in a **Microsoft Fabric-style setup**."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
