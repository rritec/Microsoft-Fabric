{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e43d910-ce88-4e4a-b03d-cc3005901705",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "`Note: ` Above 4 exercise are just for practice purpose only \n",
    "\n",
    "`Rule 1:` For just data movement donot use notebooks(spark)\n",
    "\n",
    "`In Microsoft Fabric, for data ingestion, you should strictly use the Fabric Data Pipeline’s lakehouse or Files layer ingestion capabilities, typically via Copy Data or Data Flow activities. This ensures that raw data from sources like CSV, JSON, Parquet, or databases lands in the Bronze layer of the lakehouse in a reliable, schema-compliant, and versioned manner. Using these native Fabric ingestion methods guarantees scalability, lineage tracking, and compatibility`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d595e2d-7d1f-4c8b-882f-8be1347f7fc8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 1️⃣ Read Bronze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5577112a-1bc7-4541-9896-f0e87c3bb06e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-04T14:13:24.2574512Z",
       "execution_start_time": "2026-02-04T14:13:22.0179381Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "689e8615-488e-40ba-bb62-a245651fe06c",
       "queued_time": "2026-02-04T14:13:22.0168579Z",
       "session_id": "ca95096c-3642-46f2-8b3b-689d541a980f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, ca95096c-3642-46f2-8b3b-689d541a980f, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: string (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      "\n",
      "+-----+------+--------+----+---------+----+----+------+\n",
      "|empno| ename|     job| mgr| hiredate| sal|comm|deptno|\n",
      "+-----+------+--------+----+---------+----+----+------+\n",
      "| 7369| SMITH|   CLERK|7902|17-Dec-80| 800|NULL|    20|\n",
      "| 7900| JAMES|   CLERK|7698| 3-Dec-81| 950|NULL|    30|\n",
      "| 7876| ADAMS|   CLERK|7788|23-May-87|1000|NULL|    20|\n",
      "| 7521|  WARD|SALESMAN|7698|22-Feb-81|1250| 500|    30|\n",
      "| 7654|MARTIN|SALESMAN|7698|28-Sep-81|1250|1400|    30|\n",
      "+-----+------+--------+----+---------+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, to_date, trim, coalesce, lit, when\n",
    ")\n",
    "\n",
    "# Read Bronze data from Files\n",
    "emp_bronze_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", \"true\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .csv(\"Files/bronze/emp.csv\")\n",
    ")\n",
    "\n",
    "# Preview data (Git-friendly)\n",
    "emp_bronze_df.printSchema()\n",
    "emp_bronze_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c7f3f-2a48-4efc-a298-11fce4fb32ac",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 2️⃣ Apply transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00edafa6-be7d-4125-b4ae-edeaa3f2b065",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-04T14:13:03.095342Z",
       "execution_start_time": "2026-02-04T14:13:02.2281662Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "45cee6e5-d455-4dc7-961e-3d489de87a47",
       "queued_time": "2026-02-04T14:13:02.2268863Z",
       "session_id": "ca95096c-3642-46f2-8b3b-689d541a980f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, ca95096c-3642-46f2-8b3b-689d541a980f, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: integer (nullable = true)\n",
      " |-- hiredate: date (nullable = true)\n",
      " |-- sal: integer (nullable = true)\n",
      " |-- comm: integer (nullable = false)\n",
      " |-- deptno: integer (nullable = true)\n",
      " |-- totalsal: integer (nullable = true)\n",
      " |-- salgrade: string (nullable = false)\n",
      "\n",
      "+-----+------+--------+----+----------+----+----+------+--------+--------+\n",
      "|empno| ename|     job| mgr|  hiredate| sal|comm|deptno|totalsal|salgrade|\n",
      "+-----+------+--------+----+----------+----+----+------+--------+--------+\n",
      "| 7369| SMITH|   CLERK|7902|1980-12-17| 800|   0|    20|     800|     LOW|\n",
      "| 7900| JAMES|   CLERK|7698|1981-12-03| 950|   0|    30|     950|     LOW|\n",
      "| 7876| ADAMS|   CLERK|7788|1987-05-23|1000|   0|    20|    1000|     LOW|\n",
      "| 7521|  WARD|SALESMAN|7698|1981-02-22|1250| 500|    30|    1750|     LOW|\n",
      "| 7654|MARTIN|SALESMAN|7698|1981-09-28|1250|1400|    30|    2650|  MEDIUM|\n",
      "+-----+------+--------+----+----------+----+----+------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "emp_silver_df = (\n",
    "    emp_bronze_df\n",
    "        # Convert hiredate string to DATE\n",
    "        .withColumn(\n",
    "            \"hiredate\",\n",
    "            to_date(trim(col(\"hiredate\")), \"dd-MMM-yy\")\n",
    "        )\n",
    "        # Replace NULL commission with 0\n",
    "        .withColumn(\n",
    "            \"comm\",\n",
    "            coalesce(col(\"comm\"), lit(0))\n",
    "        )\n",
    "        # Calculate total salary\n",
    "        .withColumn(\n",
    "            \"totalsal\",\n",
    "            col(\"sal\") + col(\"comm\")\n",
    "        )\n",
    "        # Derive salary grade\n",
    "        .withColumn(\n",
    "            \"salgrade\",\n",
    "            when(col(\"totalsal\") < 2000, \"LOW\")\n",
    "            .when(col(\"totalsal\").between(2000, 3400), \"MEDIUM\")\n",
    "            .otherwise(\"HIGH\")\n",
    "        )\n",
    ")\n",
    "\n",
    "# Preview transformed data\n",
    "emp_silver_df.printSchema()\n",
    "emp_silver_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7884e6b9-870d-4b28-be6a-a2e748b7a1eb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 3. Write to Silver layer (Delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec2557-fb16-4caa-a967-9ca2ee7adcb8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2026-02-04T14:14:57.1997914Z",
       "execution_start_time": "2026-02-04T14:14:43.6408067Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e6833e31-94b0-4029-9d9d-f98139d9f412",
       "queued_time": "2026-02-04T14:14:43.6397492Z",
       "session_id": "ca95096c-3642-46f2-8b3b-689d541a980f",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 17,
       "statement_ids": [
        17
       ]
      },
      "text/plain": [
       "StatementMeta(, ca95096c-3642-46f2-8b3b-689d541a980f, 17, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_silver_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"silver.silver_emp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef8996a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "a375dc6e-2dd7-4a0c-a829-7e6bc66cb0a3",
    "default_lakehouse_name": "b2026_lakehouse",
    "default_lakehouse_workspace_id": "55732739-60eb-445b-94c4-65725b7190fa",
    "known_lakehouses": [
     {
      "id": "a375dc6e-2dd7-4a0c-a829-7e6bc66cb0a3"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
