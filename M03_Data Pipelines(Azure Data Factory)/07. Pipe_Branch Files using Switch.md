# Microsoft Fabric Data Pipeline: Employee Data Ingestion

This document outlines the `EmployeeData_Ingestion_Pipeline`, a Microsoft Fabric pipeline designed to ingest employee data from different file formats (currently CSV and JSON) located in a Lakehouse and load it into a Fabric SQL Database.

## Overview

The pipeline iterates through a list of input files specified by the `pInputFiles` parameter. For each file, it determines the file extension and routes the file to the appropriate copy activity to load the data into a specific table in the Fabric SQL Database.

## Pipeline Details

**Name:** `EmployeeData_Ingestion_Pipeline`
**Object ID:** `13992b28-105b-4374-86d0-c5aeb91a33e0`

### Parameters

| Parameter Name | Type  | Default Value        | Description                                  |
|----------------|-------|----------------------|----------------------------------------------|
| `pInputFiles`  | Array | `["emp.csv", "emp.json"]` | An array of file names to be processed.     |

### Activities

The pipeline consists of the following activities:

1.  **IterateOverFiles (ForEach)**
    * **Type:** `ForEach`
    * **Dependencies:** None
    * **Description:** This activity iterates over each file name provided in the `pInputFiles` parameter.
    * **Items:** `@pipeline().parameters.pInputFiles` (Evaluates to the array of input file names)
    * **Is Sequential:** `false` (Processes files parallel)
    * **Inner Activities:**
        * `RouteFileByExtension` (Switch)

![image](https://github.com/user-attachments/assets/61d53619-97ae-403d-a25b-b1a7f1db6a29)


2.  **RouteFileByExtension (Switch)**
    * **Type:** `Switch`
    * **Dependencies:** None (within the ForEach loop, depends on the previous iteration completing)
    * **Description:** This activity routes the processing of each file based on its name (extension).
    * **On:** `@item()` (Evaluates to the current file name being processed in the ForEach loop)
    * **Cases:**
        * **Case: `emp.csv`**
            * **Activities:**
                * `Copy_CSV_To_SQL` (Copy)
        * **Case: `emp.json`**
            * **Activities:**
                * `Copy_JSON_To_SQL` (Copy)
        * **Default:** No activities defined.

![image](https://github.com/user-attachments/assets/71bd223e-3c2d-4680-9a81-75214515e7f2)


### Copy Activities

#### 1. `Copy_CSV_To_SQL`

* **Type:** `Copy`
* **Source:**
    * **Type:** `DelimitedTextSource`
    * **Linked Service:** `rr_batch100` (Lakehouse)
        * **Workspace ID:** `55732739-60eb-445b-94c4-65725b7190fa`
        * **Artifact ID:** `dd9dd813-0f22-446d-9621-dfd670945ea5`
        * **Root Folder:** `Files`
    * **Dataset:** `DelimitedText`
        * **Location:**
            * **File Name:** `emp.csv`
            * **Folder Path:** `rawdata`
        * **Column Delimiter:** `,`
        * **Escape Character:** `\`
        * **First Row as Header:** `true`
        * **Quote Character:** `"`
* **Sink:**
    * **Type:** `FabricSqlDatabaseSink`
    * **Linked Service:** `rritec` (FabricSqlDatabase)
        * **Workspace ID:** `55732739-60eb-445b-94c4-65725b7190fa`
        * **Artifact ID:** `31237005-4d13-4afe-8ff3-42834149ecd7`
        * **Connection Reference:** `cb146f64-f5ee-47c5-9a70-8bada1b07ac1`
    * **Dataset:** `FabricSqlDatabaseTable`
        * **Schema:** `dbo`
        * **Table:** `switchempcsv`
    * **Write Behavior:** `insert`
    * **Table Option:** `autoCreate`
* **Translator:** `TabularTranslator` with type conversion enabled (`allowDataTruncation`: `true`, `treatBooleanAsNumber`: `false`).

![image](https://github.com/user-attachments/assets/490bbaff-dde5-44bb-8fcf-3a977eef5155)


#### 2. `Copy_JSON_To_SQL`

* **Type:** `Copy`
* **Source:**
    * **Type:** `JsonSource`
    * **Linked Service:** `rr_batch100` (Lakehouse)
        * **Workspace ID:** `55732739-60eb-445b-94c4-65725b7190fa`
        * **Artifact ID:** `dd9dd813-0f22-446d-9621-dfd670945ea5`
        * **Root Folder:** `Files`
    * **Dataset:** `Json`
        * **Location:**
            * **File Name:** `emp.json`
            * **Folder Path:** `rawdata`
* **Sink:**
    * **Type:** `FabricSqlDatabaseSink`
    * **Linked Service:** `rritec` (FabricSqlDatabase)
        * **Workspace ID:** `55732739-60eb-445b-94c4-65725b7190fa`
        * **Artifact ID:** `31237005-4d13-4afe-8ff3-42834149ecd7`
        * **Connection Reference:** `cb146f64-f5ee-47c5-9a70-8bada1b07ac1`
    * **Dataset:** `FabricSqlDatabaseTable`
        * **Table:** `switchempjson`
    * **Write Behavior:** `insert`
    * **Table Option:** `autoCreate`
* **Translator:** `TabularTranslator` with type conversion enabled (`allowDataTruncation`: `true`, `treatBooleanAsNumber`: `false`) and column flattening settings (`treatArrayAsString`: `false`, `treatStructAsString`: `false`, `flattenColumnDelimiter`: `.`).


![image](https://github.com/user-attachments/assets/bcd0e59f-4c25-447c-9c77-1ad929eb8e1c)

## Linked Services

The pipeline utilizes the following linked services:

* **`rr_batch100` (Lakehouse):** Connects to the Microsoft Fabric Lakehouse containing the source data files in the `Files/rawdata` folder.
* **`rritec` (FabricSqlDatabase):** Connects to the Microsoft Fabric SQL Database where the employee data will be loaded into the `dbo.switchempcsv` and `dbo.switchempjson` tables.

## Datasets

The pipeline utilizes the following datasets:

* **`DelimitedText`:** Defines the schema and location of the CSV files in the Lakehouse.
* **`Json`:** Defines the schema and location of the JSON files in the Lakehouse.
* **`FabricSqlDatabaseTable` (for `switchempcsv`):** Defines the target table schema in the Fabric SQL Database for CSV data.
* **`FabricSqlDatabaseTable` (for `switchempjson`):** Defines the target table schema in the Fabric SQL Database for JSON data.

## Notes

* The pipeline assumes that the CSV files have a header row.
* The JSON data might contain nested structures, as indicated by the `columnFlattenSettings` in the `Copy_JSON_To_SQL` activity.
* The target SQL tables (`dbo.switchempcsv` and `dbo.switchempjson`) will be auto-created if they do not exist.
* Error handling and logging are not explicitly defined in this pipeline configuration and might need to be implemented for production scenarios.

