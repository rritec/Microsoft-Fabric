# üåø What is **Column Pruning** in PySpark?

**Column Pruning** is an optimization technique where **Spark reads only the columns required** for a computation ‚Äî ignoring unnecessary columns from data sources like **Parquet**, **Delta**, **ORC**, etc.

This happens automatically in Spark's **Catalyst optimizer** as part of **lazy evaluation**.

---

## üîß Why It Matters

‚úÖ **Performance Gains**:

* Reduces **I/O**: Less data is read from disk.
* Reduces **memory footprint**: Fewer columns in the DataFrame.
* Speeds up execution: Less data to shuffle or cache.

‚úÖ Especially useful for **wide tables** with dozens or hundreds of columns.

---

## üéØ Example

Assume a Parquet file with 10 columns (`id`, `name`, `email`, `country`, `status`, etc.):

```python
df = spark.read.parquet("/mnt/data/users")

# Column Pruning: only 2 columns are selected
df = df.select("id", "status")

df.show()
```

üî• **Spark only reads `id` and `status`** from disk ‚Äî other columns are ignored entirely.

---

## üß† When Does Column Pruning Work Best?

| Format         | Supports Column Pruning?     |
| -------------- | ---------------------------- |
| **Parquet**    | ‚úÖ Yes                        |
| **Delta Lake** | ‚úÖ Yes                        |
| **ORC**        | ‚úÖ Yes                        |
| **CSV / JSON** | ‚ùå No (must read all columns) |

‚ö†Ô∏è With **CSV/JSON**, Spark must read the whole file and then discard columns.

---

## üîç How to Verify Column Pruning

Use `.explain()` to inspect the **physical plan**:

```python
df.select("id", "status").explain(True)
```

‚úÖ You‚Äôll see only selected columns being scanned in the **physical plan** if pruning is applied.

---

## üí° Practical Tip

Avoid `select("*")` if you don't need all columns ‚Äî it **prevents column pruning**:

```python
# ‚ùå BAD: reads all columns
df = spark.read.parquet("/mnt/data/users").select("*")

# ‚úÖ GOOD: allows pruning
df = spark.read.parquet("/mnt/data/users").select("id", "status")
```

---

## ‚úÖ Summary

| Feature           | Column Pruning                       |
| ----------------- | ------------------------------------ |
| Optimizes I/O     | ‚úÖ Yes                                |
| Reduces Memory    | ‚úÖ Yes                                |
| Automatic?        | ‚úÖ If only necessary columns are used |
| Formats Supported | Parquet, ORC, Delta                  |

---

Would you like a Databricks notebook example with `.explain()` showing pruning in action?

**Lazy Evaluation** is one of the core optimization principles in **PySpark** and Apache Spark in general.

---

# üß† What is Lazy Evaluation in PySpark?

In **lazy evaluation**, PySpark **does not execute your code immediately** when you define transformations like `map()`, `filter()`, or `select()`. Instead, it builds up a **logical execution plan (DAG)** and only **executes** it when an **action** is called ‚Äî such as `count()`, `collect()`, or `show()`.

---

## üîÅ Lazy vs Eager Evaluation

| Aspect                 | Lazy Evaluation                    | Eager Evaluation           |
| ---------------------- | ---------------------------------- | -------------------------- |
| When is code executed? | When an **action** is triggered    | Immediately upon execution |
| Optimization?          | Yes, Spark optimizes full pipeline | No (runs line-by-line)     |
| Intermediate results   | Not materialized until needed      | Materialized at each step  |

---

## üîß Example of Lazy Evaluation

```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("LazyExample").getOrCreate()

# Create DataFrame
df = spark.range(1, 1000)

# Step 1: Transformation - this line does NOT trigger execution
df_filtered = df.filter("id % 2 == 0")

# Step 2: Another Transformation
df_squared = df_filtered.withColumn("square", df_filtered["id"] * df_filtered["id"])

# Step 3: Action - triggers the actual computation
df_squared.show()
```

### üõë What happens:

* `range()`, `filter()`, and `withColumn()` are **transformations** ‚Üí Spark builds a plan.
* Only when `.show()` (an **action**) is called, Spark executes the DAG of operations.
* The entire chain is optimized before execution (via Catalyst + Tungsten).

---

## üîç Why Lazy Evaluation Is Powerful

1. **Optimization**:

   * Spark applies **query optimizations** like predicate pushdown, column pruning, etc.
   * It skips unnecessary work and merges stages where possible.

2. **Fault Tolerance**:

   * Spark keeps the DAG, so it can **recompute** only the needed parts in case of failure.

3. **Efficiency**:

   * Avoids materializing intermediate results ‚Üí saves memory and I/O.

---

## üìä Real Use Case

```python
# Read a large CSV (no data read yet)
df = spark.read.csv("/mnt/data/large.csv", header=True, inferSchema=True)

# Filter + Transformation (still not executed)
df = df.filter("status = 'active'").select("user_id", "signup_date")

# Trigger the plan
df.write.parquet("/mnt/output/active_users")
```

* Spark won't even read the CSV until the final `.write()` is called.
* Catalyst will **combine filtering and projection** into one optimized scan.

---

## üß™ How to See the Lazy Plan?

Use `.explain()` before triggering an action:

```python
df.explain()
```

You‚Äôll see the **logical plan** and **physical plan** Spark will use.

---

## ‚úÖ Summary

| Term                             | Examples                                |
| -------------------------------- | --------------------------------------- |
| **Transformations (Lazy)**       | `select`, `filter`, `map`, `withColumn` |
| **Actions (Triggers Execution)** | `show`, `count`, `collect`, `write`     |

---

Would you like a Databricks notebook that walks through this with `.explain()` and performance tips?

Certainly! Here's a rewritten explanation of **DAG** (Directed Acyclic Graph) tailored for **Microsoft Fabric (Spark or Data Engineering context)**:

---

# üîÅ What is a DAG in Microsoft Fabric?

In Microsoft Fabric, especially when working with **Spark notebooks or Dataflows**, a **DAG (Directed Acyclic Graph)** represents the **logical flow of operations** that transform your data ‚Äî such as reading, filtering, joining, and writing.

---

### üìò Breaking it down:

* **Directed**: Each step in the workflow points to the next step ‚Äî there‚Äôs a clear direction from start to finish.
* **Acyclic**: No loops ‚Äî once a step is executed, the process does not go back.
* **Graph**: A structure where **nodes** represent operations (like reading data or applying filters) and **edges** show the dependencies between those operations.

---

### üîÑ How DAG Works in Fabric Spark Notebooks

When you define operations in a Fabric Spark notebook like:

```python
df = spark.read.parquet("Files/data.parquet")
df_filtered = df.filter("status = 'Active'")
df_result = df_filtered.groupBy("country").count()
```

* These steps **do not run immediately**.
* Fabric builds a **DAG** behind the scenes ‚Äî a plan that describes what needs to be done and in what order.
* Execution only starts when you trigger an **action**, such as:

```python
df_result.show()
```

---

### üéØ Why DAG is Important in Microsoft Fabric

‚úÖ **Optimized Execution**: Fabric can rearrange and combine steps to run them more efficiently.

‚úÖ **Fault Tolerance**: If something fails, Fabric can re-run only the necessary parts using the DAG lineage.

‚úÖ **Scalability**: The DAG helps divide the work into parallel tasks across compute nodes.

---

### ‚úÖ Summary

| Term            | Meaning in Fabric                                    |
| --------------- | ---------------------------------------------------- |
| **DAG**         | A plan of operations with dependencies and no cycles |
| **Built From**  | Transformations (e.g., filter, select, join)         |
| **Executed On** | Actions like `show()`, `write()`, etc.               |
| **Benefits**    | Optimization, fault recovery, scalability            |

---

Let me know if you‚Äôd like a DAG visualization example or how it maps to **Pipeline DAGs** in Fabric Dataflows or Data Pipelines.

Great question! In **Microsoft Fabric Spark**, **Catalyst** and **Tungsten** are two core components of Spark‚Äôs **performance engine**, working behind the scenes to optimize and execute your data transformations efficiently.

---

# ‚öôÔ∏è Catalyst vs Tungsten: Overview

| Feature           | **Catalyst**                            | **Tungsten**                           |
| ----------------- | --------------------------------------- | -------------------------------------- |
| Purpose           | **Query Optimization Framework**        | **Physical Execution Engine**          |
| Focus Area        | Optimizing SQL/DataFrame logical plans  | Speed, memory, CPU efficiency          |
| Layer             | **Logical/Planner Layer**               | **Execution Layer**                    |
| Works On          | Logical and physical query plans        | Bytecode generation, memory management |
| Optimization Type | Rule-based and cost-based optimizations | Low-level runtime optimizations        |

---

## üß† What is **Catalyst Optimizer**?

The **Catalyst** is the **query optimizer** used in Spark (and therefore in Microsoft Fabric Spark), designed to:

### ‚úÖ Tasks:

* **Parse** SQL or DataFrame code into a logical plan.
* **Analyze**: Resolves column names and types.
* **Optimize**: Applies rules like:

  * Predicate pushdown
  * Column pruning
  * Constant folding
* **Generate a physical plan** for execution.

### üßæ Example:

```python
df = spark.read.parquet("Files/sales.parquet").filter("region = 'US'")
```

Catalyst will:

* Push down the filter into the parquet scan.
* Avoid reading unnecessary columns.

---

## üî• What is **Tungsten Execution Engine**?

**Tungsten** is Spark‚Äôs **physical execution engine** focused on **performance** by optimizing:

### ‚úÖ Features:

* **Memory Management**: Uses off-heap memory to reduce garbage collection.
* **Binary Processing**: Data is stored in compact binary format for faster access.
* **Whole-Stage Code Generation (WSCG)**:

  * Dynamically generates Java bytecode to run transformations faster.
* **CPU Efficiency**: Reduces CPU instructions needed per record.

### üßæ Example:

```python
df.groupBy("region").agg(sum("sales"))
```

Tungsten ensures that groupBy and aggregation are executed in a tight, compiled loop ‚Äî not interpreted step by step.

---

## üß© How Catalyst and Tungsten Work Together in Microsoft Fabric Spark

1. **Catalyst** builds and optimizes the query plan.
2. The optimized plan is sent to **Tungsten**.
3. **Tungsten** executes that plan efficiently using compiled bytecode and optimized memory use.

---

## üß™ Analogy:

* **Catalyst** is the **planner/strategist**: ‚ÄúLet‚Äôs remove unnecessary steps, push filters early, avoid scanning extra columns.‚Äù
* **Tungsten** is the **engineer/executor**: ‚ÄúLet me do it fast with low memory and CPU cost.‚Äù

---

## ‚úÖ Summary

| Component    | Focus              | Role                                         |
| ------------ | ------------------ | -------------------------------------------- |
| **Catalyst** | Query Optimization | Builds and rewrites logical/physical plans   |
| **Tungsten** | Runtime Execution  | Runs the plan with memory and CPU efficiency |

---

Let me know if you want to see how Catalyst optimizations (like column pruning or filter pushdown) can be inspected in Microsoft Fabric notebooks using `.explain()`.

# üóëÔ∏è What is **Garbage Collection (GC)**?

**Garbage Collection** is the process by which a system **automatically reclaims memory** that is no longer in use. It identifies and removes objects in memory that are **no longer referenced** by the application, freeing up space for new data.

---

## üß† In Simple Terms:

> Garbage Collection = **Automatic memory cleanup**.

Imagine you‚Äôre done using a file or variable ‚Äî instead of deleting it manually, the system does it for you when it detects it's no longer needed.

---

## ‚öôÔ∏è Why It Matters in Microsoft Fabric (Spark Engine)

Fabric's Spark engine runs on the **Java Virtual Machine (JVM)**, where garbage collection is crucial for:

* **Releasing unused memory**
* **Preventing memory leaks**
* **Avoiding `OutOfMemoryError`**
* **Maintaining stable long-running jobs**

---

## üß™ How GC Works in JVM-Based Systems (like Spark in Fabric)

1. Spark jobs create many temporary objects (e.g., DataFrames, intermediate shuffles).
2. When those objects are no longer referenced, the JVM marks them as garbage.
3. A background process (the **GC thread**) reclaims the memory occupied by those objects.
4. Freed memory is returned to the heap for reuse.

---

## üîç GC in Spark (and Microsoft Fabric)

| GC Issue               | Impact                        | Solution                                 |
| ---------------------- | ----------------------------- | ---------------------------------------- |
| **Frequent GC pauses** | Slows down job execution      | Optimize partitions, caching, memory     |
| **Full GC events**     | Temporary stop of all threads | Tune executor memory and GC settings     |
| **Memory leaks**       | Leads to crashes              | Identify large objects, broadcast safely |

---

## üìà Monitoring Garbage Collection in Fabric

In Microsoft Fabric Spark notebooks:

* Use `%info` magic command to inspect memory and GC stats (if enabled).
* Monitor memory usage through **Spark monitoring views** (like Spark UI or Metrics).

---

## üõ†Ô∏è Tips to Reduce GC Overhead

‚úÖ **Use `.cache()` or `.persist()` wisely** ‚Äî only cache what is reused.

‚úÖ **Avoid large object serialization** (e.g., broadcasting large DataFrames without reason).

‚úÖ **Reduce number of partitions** for small data to avoid excessive metadata.

‚úÖ **Use efficient file formats** like Parquet/Delta to minimize memory usage.

---

## ‚úÖ Summary

| Concept            | Description                               |
| ------------------ | ----------------------------------------- |
| Garbage Collection | Automatic memory cleanup in the JVM       |
| Importance         | Prevents crashes, improves performance    |
| In Fabric Spark    | Managed by JVM; optimize to reduce pauses |

---

Great! Here's a breakdown of **narrow vs. wide transformations** using **PySpark code examples**, aligned with **Microsoft Fabric Spark environment**.

---

# üîπ 1. **Narrow Vs Wide Transformations Example**
## üîπ 1. **Narrow Transformations Example**

### ‚úÖ Operations: `map()`, `filter()`, `select()`

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

data = [("Alice", 2000), ("Bob", 3000), ("Charlie", 4000)]
df = spark.createDataFrame(data, ["name", "salary"])

# Narrow transformations
df_filtered = df.filter(df.salary > 2500)     # Only filters each partition locally
df_mapped = df_filtered.select("name")        # Just selects columns, no shuffle

df_mapped.show()
```

üìå **No shuffle**, works within each partition ‚Äî efficient.

---

## üîπ 2. **Wide Transformations Example**

### ‚ùå Operations: `groupBy()`, `reduceByKey()`, `join()`

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

data = [("HR", 1), ("IT", 2), ("HR", 3), ("IT", 4)]
df = spark.createDataFrame(data, ["dept", "emp_id"])

# Wide transformation: causes a shuffle across partitions
grouped_df = df.groupBy("dept").count()  # Groups by key, needs data movement

grouped_df.show()
```

üìå This triggers a **shuffle** because data from all partitions must be **grouped by key**.

---

## üß™ Example in Microsoft Fabric (SQL Analytics Endpoint for comparison)

While Spark Pool supports these transformations, in **Microsoft Fabric‚Äôs SQL Endpoint**, similar logic could be written in T-SQL (though transformations are not labeled "narrow/wide").

```sql
-- Narrow-like (local filtering)
SELECT name FROM employee WHERE salary > 2500;

-- Wide-like (requires data movement)
SELECT dept, COUNT(*) FROM employee GROUP BY dept;
```

---

## üí° How to Detect Wide Transformations?

Use the **Spark UI DAG**:

* Look for **stage boundaries** (new stages = shuffles = wide).
* **Shuffle Read/Write** = Wide transformation detected.

---

# Hash-based Merge(Kind of SCD1)

### ‚öôÔ∏è Step 1: Create Source and Target DataFrames

```python
import pandas as pd
import hashlib

# üü° Simulating existing target table
target_df = pd.DataFrame({
    'user_id': [101, 102],
    'amount': [250.00, 399.99],
    'timestamp': ['2025-07-03T03:50:00Z', '2025-07-03T03:52:10Z']
})

# üîµ Simulating new source data with updated and new rows
source_df = pd.DataFrame({
    'user_id': [101, 103],  # 101 is existing, 103 is new
    'amount': [275.00, 199.99],  # 101 has updated amount
    'timestamp': ['2025-07-03T04:01:00Z', '2025-07-03T04:05:00Z']
})

# Helper to compute hash from row
def compute_hash(row):
    raw = f"{row['user_id']}|{row['amount']}|{row['timestamp']}"
    return hashlib.sha256(raw.encode('utf-8')).hexdigest()

# Add hash column
target_df['hash'] = target_df.apply(compute_hash, axis=1)
source_df['hash'] = source_df.apply(compute_hash, axis=1)
```

---

### üîÅ Step 2: Perform MERGE Logic (Simulated)

```python
# Simulate MERGE: identify matching keys and compare hash
merged = pd.merge(source_df, target_df, on='user_id', how='left', suffixes=('_src', '_tgt'))

# Update required if hash differs
to_update = merged[~merged['hash_src'].eq(merged['hash_tgt']) & merged['hash_tgt'].notnull()]
to_insert = merged[merged['hash_tgt'].isnull()]

# Apply updates to target_df
for idx, row in to_update.iterrows():
    target_df.loc[target_df['user_id'] == row['user_id'], ['amount', 'timestamp', 'hash']] = (
        row['amount_src'], row['timestamp_src'], row['hash_src']
    )

# Insert new rows
new_rows = source_df[source_df['user_id'].isin(to_insert['user_id'])]
target_df = pd.concat([target_df, new_rows], ignore_index=True)

print(target_df)
```

---

### üí° Final Notes

- This simulates a MERGE by key (`user_id`) with change detection via `hash`.
- It updates rows only if the hash has changed.
- It inserts rows if no match is found.

---

### üß† SQL-Style MERGE (Reference Only)

```sql
MERGE INTO target_table AS tgt
USING source_table AS src
ON tgt.user_id = src.user_id
WHEN MATCHED AND tgt.hash != src.hash THEN
  UPDATE SET tgt.amount = src.amount, tgt.timestamp = src.timestamp, tgt.hash = src.hash
WHEN NOT MATCHED THEN
  INSERT (user_id, amount, timestamp, hash)
  VALUES (src.user_id, src.amount, src.timestamp, src.hash);
```

---

# Field-Level Encryption & decryption in PySpark

[Field-Level Encryption & decryption in PySpark](https://github.com/rritec/Microsoft-Fabric/blob/main/M02_02_Python%20and%20PySpark/PySpark_Field_Encryption_decryption.ipynb)

# 







